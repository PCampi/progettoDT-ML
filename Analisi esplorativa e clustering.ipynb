{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi esplorativa e clustering\n",
    "\n",
    "Questo notebook presenta l'analisi esplorativa del training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import sklearn.metrics.pairwise as pw\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "export_images = True\n",
    "style = 'white'\n",
    "img_folder = './img/'\n",
    "\n",
    "# tune this for bigger figures\n",
    "pl.rcParams['figure.figsize'] = (14, 14)\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo il dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = pd.read_excel(\"./Dataset/finali/integrato_2014.xlsx\", sheet_name='ML_finale')\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per comodità rinominiamo le colonne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = dataset_raw.rename(columns={'CO2 production (kg)': 'CO2',\n",
    "                           'Charcoal consumption (kg)': 'Charcoal',\n",
    "                           'Fuel oil consumption (kg)': 'Fuel oil',\n",
    "                           'Renewable energy consumption (percentage)': 'Clean energy',\n",
    "                           'PM2.5 (micrograms)': 'PM2.5'})\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the original dataset\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(3, 2, figsize=(18, 14))\n",
    "    plot_kde = False\n",
    "    sns.distplot(dataset_raw['CO2'], ax=ax[0, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['Charcoal'], ax=ax[0, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['Fuel oil'], ax=ax[1, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['Clean energy'], ax=ax[1, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['PM2.5'], ax=ax[2, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['GDP'], ax=ax[2, 1], kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    fig2, ax2 = pl.subplots(figsize=(18, 10))\n",
    "    sns.distplot(dataset_raw['Population'], ax=ax2, kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    pl.show()\n",
    "\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dataset-originale.png', bbox_inches='tight')\n",
    "        fig2.savefig(img_folder + 'dataset-originale-popolazione.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo le distribuzioni appaiate dei dati originali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(dataset_raw)\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-originale.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizziamo gli attributi rispetto alla popolazione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_col(dataset, ref_column=None, cols_to_normalize=None):\n",
    "    \"\"\"Divide all columns in a dataset for the normalization column.\"\"\"\n",
    "    if ref_column is None:\n",
    "        raise ValueError(\"Must choose a reference column to normalize.\")\n",
    "    if cols_to_normalize is None:\n",
    "        raise ValueError(\"Must select target columns.\")\n",
    "\n",
    "    norm_col = dataset[ref_column]\n",
    "    result = dataset.copy()\n",
    "    \n",
    "    for col in cols_to_normalize:\n",
    "        result[col] = result[col] / norm_col\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = normalize_with_col(dataset_raw,\n",
    "                             ref_column='Population',\n",
    "                             cols_to_normalize=['CO2', 'Charcoal', 'Fuel oil', 'GDP'])\n",
    "# Peek at the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiche descrittive per il dataset\n",
    "\n",
    "Il dataset contiene 153 istanze di paesi, ognuna con gli attributi\n",
    "\n",
    "- popolazione\n",
    "- produzione di CO_2 annuale (in kg)\n",
    "- consumo di carbone annuale (in kg)\n",
    "- consumo di carburanti fossili annuale (in kg)\n",
    "- percentuale di energia rinnovabile utilizzata, rispetto all'utilizzo totale di quel paese\n",
    "- GDP (prodotto interno lordo)\n",
    "\n",
    "Vediamo come sono distribuite le variabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora vediamo la distribuzione delle features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the population-scaled dataset\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(3, 2, figsize=(18, 14))\n",
    "    plot_kde = False\n",
    "    sns.distplot(dataset['CO2'], ax=ax[0, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset['Charcoal'], ax=ax[0, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset['Fuel oil'], ax=ax[1, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset['Clean energy'], ax=ax[1, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset['PM2.5'], ax=ax[2, 0], kde=plot_kde)  # TODO: sistemare labels\n",
    "    sns.distplot(dataset['GDP'], ax=ax[2, 1], kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    fig2, ax2 = pl.subplots(figsize=(18, 10))\n",
    "    sns.distplot(dataset['Population'], ax=ax2, kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    pl.show()\n",
    "\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dataset-scalato-popolazione.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(dataset, cols_to_standardize=None):\n",
    "    \"\"\"Standardize dataset.\n",
    "    \n",
    "    Returns a new copy of the dataset with the\n",
    "    selcted columns standardized.\n",
    "    \"\"\"\n",
    "    if cols_to_standardize is None:\n",
    "        raise ValueError(\"No column passed for standardization\")\n",
    "\n",
    "    result = dataset.copy()\n",
    "    for col in cols_to_standardize:\n",
    "        vals = preprocessing.scale(dataset[col].values)\n",
    "        result.loc[:, col] = vals\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_std = standardize(dataset,\n",
    "                          cols_to_standardize=[\n",
    "                              'Population',\n",
    "                              'CO2',\n",
    "                              'Charcoal',\n",
    "                              'Fuel oil',\n",
    "                              'Clean energy',\n",
    "                              'PM2.5',\n",
    "                              'GDP'])\n",
    "dataset_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the standardized population-scaled dataset\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(3, 2, figsize=(18, 14))\n",
    "    plot_kde = False\n",
    "    sns.distplot(dataset_std['CO2'], ax=ax[0, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['Charcoal'], ax=ax[0, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['Fuel oil'], ax=ax[1, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['Clean energy'], ax=ax[1, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['PM2.5'], ax=ax[2, 0], kde=plot_kde)  # TODO: sistemare labels\n",
    "    sns.distplot(dataset_std['GDP'], ax=ax[2, 1], kde=plot_kde)\n",
    "    sns.despine()\n",
    "    pl.show()\n",
    "\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dataset-std.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(dataset_std.loc[:,\n",
    "                                           ['CO2',\n",
    "                                           'Charcoal',\n",
    "                                           'Fuel oil',\n",
    "                                           'Clean energy',\n",
    "                                           'PM2.5',\n",
    "                                           'GDP']])\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-standardizzato.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlazione tra features\n",
    "\n",
    "Esploriamo la correlazione tra le features con una heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(dataset, figsize=(14, 14)):\n",
    "    \"\"\"Create a heatmap from the dataset.\"\"\"\n",
    "    # Compute the correlation matrix\n",
    "    corr = dataset.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = pl.subplots(figsize=figsize)\n",
    "    \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.color_palette(\"RdBu\")\n",
    "    \n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    hm = sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, center=0, cmap=cmap, annot=True,\n",
    "                     square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_images:\n",
    "    sns.set_context('talk')\n",
    "    heatmap = create_heatmap(dataset_raw)\n",
    "    fig = heatmap.figure\n",
    "    fig.savefig(img_folder + 'correlazione.png', bbox_inches='tight')\n",
    "    sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means con Scikit-Learn\n",
    "\n",
    "Ora che abbiamo esplotato il dataset, utilizziamo Sklearn per indurre un modello K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selezione delle feature\n",
    "\n",
    "Da quanto emerge dall'analisi esplorativa, la feature \"Fuel Oil Consumption\" è estremamente sbilanciata verso lo zero, con solo l'Afghanistan a superare quota 10 kg per-capita.\n",
    "\n",
    "In base a ciò, si è deciso di **eliminare la feature e ritenere soltanto le altre**, scalate rispetto alla popolazione e standardizzate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km = dataset_std.copy()\n",
    "data_km = data_km.drop(columns=['Fuel oil', 'Population'])\n",
    "data_km.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise plot delle feature scelte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(data_km, size=2.5)\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-KMeans.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Ora che il dataset è standardizzato, procediamo a clusterizzare.\n",
    "\n",
    "Per trovare la migliore clusterizzazoine, creiamo una funzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(X, min_clusters, max_clusters):\n",
    "    n_clusters = list(range(min_clusters, max_clusters + 1))\n",
    "    \n",
    "    # (i, j) -> label for data point i when using n_clusters[j] clusters\n",
    "    labels = np.zeros((X.shape[0], len(n_clusters)), dtype=np.int16)\n",
    "    \n",
    "    # (0, j) -> average silhouette score when using n_clusters[j] clusters\n",
    "    silh_tot = np.zeros((1, len(n_clusters)))\n",
    "    \n",
    "    # (i, j) -> silhouette score for data point i when using n_clusters[j] clusters\n",
    "    silh_ith = np.zeros((X.shape[0], len(n_clusters)))\n",
    "    \n",
    "    for ind, n in enumerate(tqdm.tqdm_notebook(n_clusters)):\n",
    "        clusterer = KMeans(n_clusters=n, init='k-means++', n_init=50,\n",
    "                           max_iter=100000, tol=1e-7,\n",
    "                           precompute_distances=True, random_state=10,\n",
    "                           n_jobs=1)\n",
    "        \n",
    "        curr_labels = clusterer.fit_predict(X)\n",
    "        labels[:, ind] = curr_labels\n",
    "\n",
    "        silhouette_avg = silhouette_score(X, curr_labels)\n",
    "        silh_tot[0, ind] = silhouette_avg\n",
    "\n",
    "        curr_silhouette_values = silhouette_samples(X, curr_labels)\n",
    "        silh_ith[:, ind] = curr_silhouette_values\n",
    "\n",
    "    ret_labels = pd.DataFrame(data=labels, columns=n_clusters)\n",
    "    ret_silh_avg = pd.DataFrame(data=silh_tot, columns=n_clusters)\n",
    "    ret_silh_point = pd.DataFrame(data=silh_ith, columns=n_clusters)\n",
    "\n",
    "    return ret_labels, ret_silh_avg, ret_silh_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora lanciamo il K-Means su un numero variabile di clusters, da 2 a 100. Vediamo poi quale è il migliore con l'indice di silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_values = data_km[['CO2', 'Charcoal', 'Clean energy', 'PM2.5', 'GDP']].values\n",
    "labels, avg_silhouette, point_silhouette = clusterize(data_values, 2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(avg_silhouette.values)\n",
    "best_silhouette = avg_silhouette.iloc[0, best_index]\n",
    "best_n_cluster = avg_silhouette.columns[best_index]\n",
    "\n",
    "best_cluster_labels = labels[labels.columns[best_index]].values\n",
    "\n",
    "print(\"Best silhouette score is {:6.4f} with {} clusters\"\n",
    "     .format(best_silhouette, best_n_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot andamento silhouette media con il numero di clusters\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    avg_silh_plot = avg_silhouette.loc[0, :].values\n",
    "    fig, ax = pl.subplots(ncols=1, figsize=(14, 10))\n",
    "    pl.plot(np.array(avg_silhouette.columns.tolist()), avg_silh_plot)\n",
    "    ax.set_title(\"Silhouette score\")\n",
    "    sns.despine()\n",
    "    \n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'silhouette_vs_nclusters.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot dei valori di silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo una funzione per plottare la silhouette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that outputs the silhouette values for each cluster\n",
    "def silhouette_for_clusters(labels, silhouettes):\n",
    "    if len(labels) != len(silhouettes):\n",
    "        raise ValueError(\"Lenght of labels ({}) differs from length of silhouettes ({})\"\n",
    "                        .format(len(labels), len(silhouettes)))\n",
    "\n",
    "    cluster_names, cluster_size = np.unique(labels, return_counts=True)\n",
    "    cluster_silh = {n: np.mean(silhouettes[labels == n]) for n in cluster_names}\n",
    "    \n",
    "    return cluster_silh, cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_silh, cluster_size = silhouette_for_clusters(\n",
    "    best_cluster_labels,\n",
    "    point_silhouette[best_n_cluster].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ci sono {} clusters.\\n\".format(len(cluster_size)))\n",
    "for ind, size in enumerate(cluster_size):\n",
    "        print(\"Cluster {} ha {:3d} elementi e silhouette = {:6.4f}\".format(ind, size, cluster_silh[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(cluster_labels, avg_silh, point_silh, cluster_silh, n_clusters, fig, ax, context):\n",
    "    \"\"\"Make a silhouette plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        data corresponding to the plots\n",
    "        \n",
    "    cluster_labels: np.array of shape (n_points, 1)\n",
    "        labels of the cluster for each data point\n",
    "    \n",
    "    point_silh: np.array (n_points, 1)\n",
    "        silhouette for each data point at the defined cluster number n_clusters\n",
    "    \n",
    "    avg_silh: float\n",
    "        average silhouette score for this clusterization\n",
    "    \"\"\"\n",
    "    min_silh_score = point_silh.min()\n",
    "    max_silh_score = point_silh.max()\n",
    "    \n",
    "    ax.set_xlim([min_silh_score, max_silh_score])\n",
    "    ax.set_ylim([0, len(cluster_labels) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    y_lower = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        # aggregate silhouette by cluster label and sort it\n",
    "        silh_values_cluster_i = point_silh[cluster_labels == i]\n",
    "        silh_values_cluster_i.sort()\n",
    "        \n",
    "        ith_cluster_size = silh_values_cluster_i.shape[0]\n",
    "        y_upper = y_lower + ith_cluster_size\n",
    "        \n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, silh_values_cluster_i,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        # Label with the cluster number in the middle\n",
    "        text_x = -0.09 if context != 'talk' else -0.11\n",
    "        text = \"Cluster {}\\nsilhouette = {:4.2f}\".format(str(i), cluster_silh[i])\n",
    "        ax.text(text_x, y_lower + 0.5 * ith_cluster_size, text)\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax.set_title(\"Silhouette plot con {} clusters\".format(n_clusters))\n",
    "#    ax.set_yticks([])\n",
    "    ax.axvline(x=avg_silh, color='blue', linestyle='--')\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(ncols=1, figsize=(16, 20))\n",
    "    plot_silhouette(labels[best_n_cluster].values,\n",
    "                    avg_silhouette[best_n_cluster].values,\n",
    "                    point_silhouette[best_n_cluster].values,\n",
    "                    cluster_silh,\n",
    "                    best_n_cluster,\n",
    "                    fig, ax, 'talk')\n",
    "    sns.despine()\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'Silhouette.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungo la colonna con i labels al dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_km.copy()\n",
    "data_final['cluster'] = best_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(data_final,\n",
    "                            vars=['CO2', 'Charcoal', 'Clean energy', 'PM2.5', 'GDP'],\n",
    "                            hue='cluster')\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-KMeans-con-clusters.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_matrix(data, output='distance'):\n",
    "    \"\"\"Create a dissimilarity matrix from the input data,\n",
    "    considered as (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    distances = pw.pairwise_distances(data, metric='euclidean', n_jobs=1)\n",
    "    similarity = np.max(distances) - distances\n",
    "    return similarity\n",
    "    \n",
    "def create_distance_matrix(data):\n",
    "    \"\"\"Create a distance matrix from the data, considered as (n_samples, n_features).\"\"\"\n",
    "    distances = pw.pairwise_distances(data, metric='euclidean', n_jobs=1)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la matrice di dissimilarità (o delle distanze):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix = create_distance_matrix(data_values)\n",
    "similarity_matrix = create_similarity_matrix(data_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo la matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_values is the variable containing the data to measure dissimilarity\n",
    "fig, ax = pl.subplots(figsize=(16, 16))\n",
    "sns.heatmap(dissimilarity_matrix, square=True,\n",
    "            cmap=sns.cubehelix_palette(8, start=0.5, rot=-0.75), ax=ax)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_values is the variable containing the data to measure similarity\n",
    "fig, ax = pl.subplots(figsize=(16, 16))\n",
    "sns.heatmap(similarity_matrix, square=True,\n",
    "            cmap=sns.cubehelix_palette(8, start=0.5, rot=-0.75), ax=ax)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi PCA per la varianza spiegata\n",
    "\n",
    "Vediamo il contributo di ogni feature alla varianza spiegata dei dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "X_pca = pca.fit_transform(dataset_std.loc[:,['CO2',\n",
    "                                             'Charcoal',\n",
    "                                             'Clean energy',\n",
    "                                             'PM2.5',\n",
    "                                             'GDP']])\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(pca_obj, threshold=0.9, despine=False):\n",
    "    n = len(pca_obj.explained_variance_ratio_)\n",
    "    x_vals = np.arange(n)\n",
    "    \n",
    "    # bar chart\n",
    "    pl.bar(x_vals,\n",
    "           pca_obj.explained_variance_ratio_,\n",
    "           alpha=0.5,\n",
    "           align=\"center\",\n",
    "           label=\"Varianza spiegata attributo\")\n",
    "    \n",
    "    # step plot\n",
    "    pl.step(x_vals,\n",
    "            np.cumsum(pca_obj.explained_variance_ratio_),\n",
    "            where=\"mid\",\n",
    "            label=\"Varianza spiegata cumulata\")\n",
    "    \n",
    "    # threshold\n",
    "    p = pl.plot(x_vals,\n",
    "           threshold * np.ones(x_vals.shape),\n",
    "           linestyle='--',\n",
    "           label=\"Soglia al {}%\".format(threshold * 100))\n",
    "    \n",
    "    pl.xlabel(\"Componenti principali\")\n",
    "    pl.ylabel(\"Rapporto varianza spiegata\")\n",
    "    pl.legend(loc=\"center right\")\n",
    "    if despine:\n",
    "        sns.despine()\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "        \n",
    "    fig, ax = pl.subplots(ncols=1, figsize=(12, 10))\n",
    "    plot_explained_variance(pca, threshold=0.95, despine=True)\n",
    "    \n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'varianza-spiegata.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
