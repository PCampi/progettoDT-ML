{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi esplorativa e clustering\n",
    "\n",
    "Questo notebook presenta l'analisi esplorativa del training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import sklearn.metrics.pairwise as pw\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "export_images = True\n",
    "style = 'white'\n",
    "img_folder = './img/'\n",
    "\n",
    "# tune this for bigger figures\n",
    "pl.rcParams['figure.figsize'] = (14, 14)\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo il dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = pd.read_excel(\"./Dataset/finali/integrato_2014.xlsx\", sheet_name='ML_finale')\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per comodit√† rinominiamo le colonne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = dataset_raw.rename(columns={'CO2 production (kg)': 'CO2',\n",
    "                           'Charcoal consumption (kg)': 'Charcoal',\n",
    "                           'Fuel oil consumption (kg)': 'Fuel oil',\n",
    "                           'Renewable energy consumption (percentage)': 'Clean energy',\n",
    "                           'PM2.5 (micrograms)': 'PM2.5'})\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the original dataset\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(3, 2, figsize=(18, 14))\n",
    "    plot_kde = False\n",
    "    sns.distplot(dataset_raw['CO2'], ax=ax[0, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['Charcoal'], ax=ax[0, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['Fuel oil'], ax=ax[1, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['Clean energy'], ax=ax[1, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['PM2.5'], ax=ax[2, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_raw['GDP'], ax=ax[2, 1], kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    fig2, ax2 = pl.subplots(figsize=(18, 10))\n",
    "    sns.distplot(dataset_raw['Population'], ax=ax2, kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    pl.show()\n",
    "\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dataset-originale.png', bbox_inches='tight')\n",
    "        fig2.savefig(img_folder + 'dataset-originale-popolazione.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo le distribuzioni appaiate dei dati originali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(dataset_raw)\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-originale.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizziamo gli attributi rispetto alla popolazione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_col(dataset, ref_column=None, cols_to_normalize=None):\n",
    "    \"\"\"Divide all columns in a dataset for the normalization column.\"\"\"\n",
    "    if ref_column is None:\n",
    "        raise ValueError(\"Must choose a reference column to normalize.\")\n",
    "    if cols_to_normalize is None:\n",
    "        raise ValueError(\"Must select target columns.\")\n",
    "\n",
    "    norm_col = dataset[ref_column]\n",
    "    result = dataset.copy()\n",
    "    \n",
    "    for col in cols_to_normalize:\n",
    "        result[col] = result[col] / norm_col\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = normalize_with_col(dataset_raw,\n",
    "                             ref_column='Population',\n",
    "                             cols_to_normalize=['CO2', 'Charcoal', 'Fuel oil', 'GDP'])\n",
    "# Peek at the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiche descrittive per il dataset\n",
    "\n",
    "Il dataset contiene 153 istanze di paesi, ognuna con gli attributi\n",
    "\n",
    "- popolazione\n",
    "- produzione di CO_2 annuale (in kg)\n",
    "- consumo di carbone annuale (in kg)\n",
    "- consumo di carburanti fossili annuale (in kg)\n",
    "- percentuale di energia rinnovabile utilizzata, rispetto all'utilizzo totale di quel paese\n",
    "- GDP (prodotto interno lordo)\n",
    "\n",
    "Vediamo come sono distribuite le variabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descrizione del vero dataset utilizzato per clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:, 3:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora vediamo la distribuzione delle features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the population-scaled dataset\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(3, 2, figsize=(18, 14))\n",
    "    plot_kde = False\n",
    "    sns.distplot(dataset['CO2'], ax=ax[0, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset['Charcoal'], ax=ax[0, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset['Fuel oil'], ax=ax[1, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset['Clean energy'], ax=ax[1, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset['PM2.5'], ax=ax[2, 0], kde=plot_kde)  # TODO: sistemare labels\n",
    "    sns.distplot(dataset['GDP'], ax=ax[2, 1], kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    fig2, ax2 = pl.subplots(figsize=(18, 10))\n",
    "    sns.distplot(dataset['Population'], ax=ax2, kde=plot_kde)\n",
    "    sns.despine()\n",
    "    \n",
    "    pl.show()\n",
    "\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dataset-scalato-popolazione.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(dataset, cols_to_standardize=None):\n",
    "    \"\"\"Standardize dataset.\n",
    "    \n",
    "    Returns a new copy of the dataset with the\n",
    "    selcted columns standardized.\n",
    "    \"\"\"\n",
    "    if cols_to_standardize is None:\n",
    "        raise ValueError(\"No column passed for standardization\")\n",
    "\n",
    "    result = dataset.copy()\n",
    "    for col in cols_to_standardize:\n",
    "        vals = preprocessing.scale(dataset[col].values)\n",
    "        result.loc[:, col] = vals\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_std = standardize(dataset,\n",
    "                          cols_to_standardize=[\n",
    "                              'Population',\n",
    "                              'CO2',\n",
    "                              'Charcoal',\n",
    "                              'Fuel oil',\n",
    "                              'Clean energy',\n",
    "                              'PM2.5',\n",
    "                              'GDP'])\n",
    "dataset_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the standardized population-scaled dataset\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(3, 2, figsize=(18, 14))\n",
    "    plot_kde = False\n",
    "    sns.distplot(dataset_std['CO2'], ax=ax[0, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['Charcoal'], ax=ax[0, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['Fuel oil'], ax=ax[1, 0], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['Clean energy'], ax=ax[1, 1], kde=plot_kde)\n",
    "    sns.distplot(dataset_std['PM2.5'], ax=ax[2, 0], kde=plot_kde)  # TODO: sistemare labels\n",
    "    sns.distplot(dataset_std['GDP'], ax=ax[2, 1], kde=plot_kde)\n",
    "    sns.despine()\n",
    "    pl.show()\n",
    "\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dataset-std.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot del dataset standardizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(dataset_std.loc[:,\n",
    "                                           ['CO2',\n",
    "                                           'Charcoal',\n",
    "                                           'Fuel oil',\n",
    "                                           'Clean energy',\n",
    "                                           'PM2.5',\n",
    "                                           'GDP']])\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-std.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot del dataset non standardizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(dataset.loc[:,\n",
    "                                           ['CO2',\n",
    "                                           'Charcoal',\n",
    "                                           'Fuel oil',\n",
    "                                           'Clean energy',\n",
    "                                           'PM2.5',\n",
    "                                           'GDP']])\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-non-std.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlazione tra features\n",
    "\n",
    "Esploriamo la correlazione tra le features con una heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(dataset, figsize=(14, 14)):\n",
    "    \"\"\"Create a heatmap from the dataset.\"\"\"\n",
    "    # Compute the correlation matrix\n",
    "    corr = dataset.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = pl.subplots(figsize=figsize)\n",
    "    \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.color_palette(\"RdBu\")\n",
    "    \n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    hm = sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, center=0, cmap=cmap, annot=True,\n",
    "                     square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_images:\n",
    "    sns.set_context('poster')\n",
    "    \n",
    "heatmap = create_heatmap(dataset)\n",
    "\n",
    "if export_images:\n",
    "    fig = heatmap.figure\n",
    "    fig.savefig(img_folder + 'correlazione.png', bbox_inches='tight')\n",
    "    sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means con Scikit-Learn\n",
    "\n",
    "Ora che abbiamo esplotato il dataset, utilizziamo Sklearn per indurre un modello K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selezione delle feature\n",
    "\n",
    "Da quanto emerge dall'analisi esplorativa, la feature \"Fuel Oil Consumption\" √® estremamente sbilanciata verso lo zero, con solo l'Afghanistan a superare quota 10 kg per-capita.\n",
    "\n",
    "In base a ci√≤, si √® deciso di **eliminare la feature e ritenere soltanto le altre**, scalate rispetto alla popolazione e **standardizzate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km = dataset_std.copy()\n",
    "data_km = data_km.drop(columns=['Fuel oil', 'Population'])\n",
    "data_km.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come esperimento comparativo, teniamo anche un dataset non standardizzato per il clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km_non_std = dataset.copy()\n",
    "data_km_non_std = data_km_non_std.drop(columns=['Fuel oil', 'Population'])\n",
    "data_km_non_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise plot delle feature scelte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(data_km, size=2.5)\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-KMeans.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Ora che il dataset √® standardizzato, procediamo a clusterizzare.\n",
    "\n",
    "Per trovare la migliore clusterizzazoine, creiamo una funzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(X, min_clusters, max_clusters):\n",
    "    n_clusters = list(range(min_clusters, max_clusters + 1))\n",
    "    \n",
    "    # (i, j) -> label for data point i when using n_clusters[j] clusters\n",
    "    labels = np.zeros((X.shape[0], len(n_clusters)), dtype=np.int16)\n",
    "    \n",
    "    # (0, j) -> average silhouette score when using n_clusters[j] clusters\n",
    "    silh_tot = np.zeros((1, len(n_clusters)))\n",
    "    \n",
    "    # (i, j) -> silhouette score for data point i when using n_clusters[j] clusters\n",
    "    silh_ith = np.zeros((X.shape[0], len(n_clusters)))\n",
    "    \n",
    "    centroids = dict()\n",
    "    inertia = dict()\n",
    "    \n",
    "    for ind, n in enumerate(tqdm.tqdm_notebook(n_clusters)):\n",
    "        clusterer = KMeans(n_clusters=n, init='k-means++', n_init=10,\n",
    "                           max_iter=100000, tol=1e-7,\n",
    "                           precompute_distances=True, random_state=10,\n",
    "                           n_jobs=1)\n",
    "        \n",
    "        curr_labels = clusterer.fit_predict(X)\n",
    "        labels[:, ind] = curr_labels\n",
    "\n",
    "        silhouette_avg = silhouette_score(X, curr_labels)\n",
    "        silh_tot[0, ind] = silhouette_avg\n",
    "\n",
    "        curr_silhouette_values = silhouette_samples(X, curr_labels)\n",
    "        silh_ith[:, ind] = curr_silhouette_values\n",
    "        \n",
    "        centroids[n] = clusterer.cluster_centers_\n",
    "        inertia[n] = clusterer.inertia_\n",
    "\n",
    "    ret_labels = pd.DataFrame(data=labels, columns=n_clusters)\n",
    "    ret_silh_avg = pd.DataFrame(data=silh_tot, columns=n_clusters)\n",
    "    ret_silh_point = pd.DataFrame(data=silh_ith, columns=n_clusters)\n",
    "\n",
    "    return ret_labels, ret_silh_avg, ret_silh_point, centroids, inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora lanciamo il K-Means su un numero variabile di clusters, da 2 a 100. Vediamo poi quale √® il migliore con l'indice di silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_values = data_km[['CO2', 'Charcoal', 'Clean energy', 'PM2.5', 'GDP']].values\n",
    "labels, avg_silhouette, point_silhouette, centroids, inertia = clusterize(data_values, 2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(avg_silhouette.values)\n",
    "best_silhouette = avg_silhouette.iloc[0, best_index]\n",
    "best_n_cluster = avg_silhouette.columns[best_index]\n",
    "best_centroids = centroids[best_n_cluster]\n",
    "best_inertia = inertia[best_n_cluster]\n",
    "\n",
    "best_cluster_labels = labels[labels.columns[best_index]].values\n",
    "\n",
    "print(\"Standardized clustering\")\n",
    "print(\"Best silhouette score is {:6.4f} with {} clusters\"\n",
    "     .format(best_silhouette, best_n_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot andamento silhouette media con il numero di clusters\n",
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('poster')\n",
    "\n",
    "    avg_silh_plot = avg_silhouette.loc[0, :].values\n",
    "    fig, ax = pl.subplots(ncols=1, figsize=(14, 10))\n",
    "    pl.plot(np.array(avg_silhouette.columns.tolist()), avg_silh_plot)\n",
    "    ax.axvline(x=avg_silhouette.columns.tolist()[np.argmax(avg_silh_plot)], color='paleturquoise', linestyle='--')\n",
    "    ax.text(np.argmax(avg_silh_plot) + 2.5, np.max(avg_silh_plot), \"Best n_cluster = 4\")\n",
    "    pl.xlabel(\"Numero di clusters\")\n",
    "    pl.ylabel(\"Silhouette\")\n",
    "    ax.set_title(\"Silhouette score\")\n",
    "    sns.despine()\n",
    "    \n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'silhouette_vs_nclusters.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot dei valori di silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo una funzione per plottare la silhouette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that outputs the silhouette values for each cluster\n",
    "def silhouette_for_clusters(labels, silhouettes):\n",
    "    if len(labels) != len(silhouettes):\n",
    "        raise ValueError(\"Lenght of labels ({}) differs from length of silhouettes ({})\"\n",
    "                        .format(len(labels), len(silhouettes)))\n",
    "\n",
    "    cluster_names, cluster_size = np.unique(labels, return_counts=True)\n",
    "    cluster_silh = {n: np.mean(silhouettes[labels == n]) for n in cluster_names}\n",
    "    \n",
    "    return cluster_silh, cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_silh, cluster_size = silhouette_for_clusters(\n",
    "    best_cluster_labels,\n",
    "    point_silhouette[best_n_cluster].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ci sono {} clusters per il dataset standardizzato.\\n\".format(len(cluster_size)))\n",
    "for ind, size in enumerate(cluster_size):\n",
    "        print(\"Cluster {} ha {:3d} elementi e silhouette = {:6.4f}\".format(ind, size, cluster_silh[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(cluster_labels, avg_silh, point_silh, cluster_silh, n_clusters, fig, ax, context, palette=None):\n",
    "    \"\"\"Make a silhouette plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        data corresponding to the plots\n",
    "        \n",
    "    cluster_labels: np.array of shape (n_points, 1)\n",
    "        labels of the cluster for each data point\n",
    "    \n",
    "    point_silh: np.array (n_points, 1)\n",
    "        silhouette for each data point at the defined cluster number n_clusters\n",
    "    \n",
    "    avg_silh: float\n",
    "        average silhouette score for this clusterization\n",
    "    \"\"\"\n",
    "    min_silh_score = point_silh.min()\n",
    "    max_silh_score = point_silh.max()\n",
    "    \n",
    "    ax.set_xlim([min_silh_score, max_silh_score])\n",
    "    ax.set_ylim([0, len(cluster_labels) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    y_lower = 10\n",
    "    if not palette:\n",
    "        palette = sns.color_palette('pastel', n_clusters)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        # aggregate silhouette by cluster label and sort it\n",
    "        silh_values_cluster_i = point_silh[cluster_labels == i]\n",
    "        silh_values_cluster_i.sort()\n",
    "        \n",
    "        ith_cluster_size = silh_values_cluster_i.shape[0]\n",
    "        y_upper = y_lower + ith_cluster_size\n",
    "        \n",
    "        color = palette[i]#cm.spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, silh_values_cluster_i,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        # Label with the cluster number in the middle\n",
    "        text_x = -0.09 if context != 'talk' else -0.11\n",
    "        text = \"Cluster {}\\nsilhouette = {:4.2f}\".format(str(i), cluster_silh[i])\n",
    "        ax.text(text_x, y_lower + 0.5 * ith_cluster_size, text)\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax.set_title(\"Silhouette plot con {} clusters\".format(n_clusters))\n",
    "#    ax.set_yticks([])\n",
    "    ax.axvline(x=avg_silh, color='paleturquoise', linestyle='--')\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo la silhouette dei cluster sul dataset standardizzato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "\n",
    "    fig, ax = pl.subplots(ncols=1, figsize=(16, 20))\n",
    "    plot_silhouette(labels[best_n_cluster].values,\n",
    "                    avg_silhouette[best_n_cluster].values,\n",
    "                    point_silhouette[best_n_cluster].values,\n",
    "                    cluster_silh,\n",
    "                    best_n_cluster,\n",
    "                    fig, ax, 'talk',\n",
    "                   palette=sns.color_palette('muted', best_n_cluster))\n",
    "    sns.despine()\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'Silhouette.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungo la colonna con i labels al dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_km.copy()\n",
    "data_final['cluster'] = best_cluster_labels\n",
    "data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('talk')\n",
    "    pairplot = sns.pairplot(data_final,\n",
    "                            vars=['CO2', 'Charcoal', 'Clean energy', 'PM2.5', 'GDP'],\n",
    "                            hue='cluster')\n",
    "    pl.show()\n",
    "    \n",
    "    if export_images:\n",
    "        fig = pairplot.fig\n",
    "        fig.savefig(img_folder + 'pairplot-KMeans-con-clusters.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in np.unique(data_final['cluster'].values):\n",
    "    print(\"#-------------------------#\")\n",
    "    cluster_k = data_final[data_final['cluster'] == k]\n",
    "    print(\"Cluster {} con {} elementi\\n\".format(str(k), str(len(cluster_k))))\n",
    "    for country in cluster_k['Country name']:\n",
    "        print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice di dissimilarit√†\n",
    "\n",
    "Vediamo ora la matrice di dissimilarit√†."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funzione per ordinare i dati a seconda del loro cluster, e all'interno di ogni cluster in base alla distanza dal centroide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prova(data, centroids):\n",
    "    \"\"\"\n",
    "    data: pd.DataFrame\n",
    "        contains one data instance per row, and a cloumn 'cluster'\n",
    "        which gives the cluster label for the instance.\n",
    "        Must not contain NaNs or None values, else the algorithm fails.\n",
    "    \"\"\"\n",
    "    cluster_labels = np.unique(data['cluster'].values)\n",
    "    \n",
    "    new_data = data.sort_values(by='cluster',\n",
    "                                axis='index',\n",
    "                                ascending=True,\n",
    "                                inplace=False)\n",
    "    tmp = []\n",
    "    for i in cluster_labels:\n",
    "        # 1. select the centroid\n",
    "        centroid = centroids[i, :]\n",
    "        centroid = centroid.reshape((1, centroid.shape[0]))\n",
    "        \n",
    "        # 2. select the data in the i-th cluster\n",
    "        data_in_cluster_i = new_data[new_data['cluster'] == i]  # all the original data where cluster = i\n",
    "        data_for_distance = data_in_cluster_i.iloc[:, 2:-1]\n",
    "        \n",
    "        # 3. compute distances from centroids\n",
    "        distances_from_centroid = euclidean_distances(data_for_distance.values, centroid)\n",
    "        \n",
    "        # 4. sort the data in the cluster by their distance from the centroid\n",
    "        data_to_sort = data_in_cluster_i\n",
    "        data_to_sort['distance from centroid'] = distances_from_centroid\n",
    "        data_sorted = data_to_sort.sort_values(by='distance from centroid')\n",
    "        \n",
    "        tmp.append(data_sorted)\n",
    "    \n",
    "    result = tmp[0]\n",
    "    for i in range(1, len(tmp)):\n",
    "        result = result.append(tmp[i])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_for_plot = prova(data_final, best_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_matrix(data):\n",
    "    \"\"\"Create a dissimilarity matrix from the input data,\n",
    "    considered as (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    distances = pw.pairwise_distances(data, metric='euclidean', n_jobs=1)\n",
    "    similarity = np.max(distances) - distances\n",
    "    return similarity\n",
    "    \n",
    "def create_distance_matrix(data):\n",
    "    \"\"\"Create a distance matrix from the data, considered as (n_samples, n_features).\"\"\"\n",
    "    distances = pw.pairwise_distances(data, metric='euclidean', n_jobs=1)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la matrice di dissimilarit√† (o delle distanze):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_subset = data_for_plot.iloc[:, 2:-2]\n",
    "to_plot_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix = create_distance_matrix(to_plot_subset.values)\n",
    "similarity_matrix = create_similarity_matrix(to_plot_subset.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo la matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_heatmap = pd.DataFrame(data=dissimilarity_matrix,\n",
    "                                     index=data_for_plot['Country name'],\n",
    "                                    columns=data_for_plot['Country name'])\n",
    "dataframe_for_heatmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_values is the variable containing the data to measure dissimilarity\n",
    "#colormap = sns.color_palette('BuGn')\n",
    "colormap = sns.cubehelix_palette(25, start=0.5, rot=-0.75)\n",
    "show_countries = False\n",
    "\n",
    "fig, ax = pl.subplots(figsize=(20, 20))\n",
    "if show_countries:\n",
    "    sns.heatmap(dataframe_for_heatmap,\n",
    "                square=True,\n",
    "                cmap=colormap,\n",
    "                xticklabels=dataframe_for_heatmap.columns.tolist(),\n",
    "                yticklabels=dataframe_for_heatmap.columns.tolist(),\n",
    "                ax=ax)\n",
    "    pl.show()\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dissimilarity-matrix-countries.png', bbox_inches='tight')\n",
    "else:\n",
    "    if export_images:\n",
    "        sns.set_context('poster')\n",
    "    sns.heatmap(dataframe_for_heatmap,\n",
    "                square=True,\n",
    "                cmap=colormap,\n",
    "                xticklabels=7,\n",
    "                yticklabels=7,\n",
    "                ax=ax)\n",
    "    pl.show()\n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'dissimilarity-matrix-no-countries.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi PCA per la varianza spiegata\n",
    "\n",
    "Vediamo il contributo di ogni feature alla varianza spiegata dei dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "X_pca = pca.fit_transform(dataset_std.loc[:,['CO2',\n",
    "                                             'Charcoal',\n",
    "                                             'Clean energy',\n",
    "                                             'PM2.5',\n",
    "                                             'GDP']])\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ns = PCA(n_components=None)\n",
    "X_pca_ns = pca_ns.fit_transform(dataset.loc[:,['CO2',\n",
    "                                             'Charcoal',\n",
    "                                             'Clean energy',\n",
    "                                             'PM2.5',\n",
    "                                             'GDP']])\n",
    "pca_ns.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(pca_obj, threshold=0.9, despine=False):\n",
    "    n = len(pca_obj.explained_variance_ratio_)\n",
    "    x_vals = np.arange(n)\n",
    "    \n",
    "    # bar chart\n",
    "    pl.bar(x_vals,\n",
    "           pca_obj.explained_variance_ratio_,\n",
    "           alpha=0.5,\n",
    "           align=\"center\",\n",
    "           label=\"Varianza spiegata attributo\")\n",
    "    \n",
    "    # step plot\n",
    "    pl.step(x_vals,\n",
    "            np.cumsum(pca_obj.explained_variance_ratio_),\n",
    "            where=\"mid\",\n",
    "            label=\"Varianza spiegata cumulata\")\n",
    "    \n",
    "    # threshold\n",
    "    p = pl.plot(x_vals,\n",
    "           threshold * np.ones(x_vals.shape),\n",
    "           linestyle='--',\n",
    "           label=\"Soglia al {}%\".format(threshold * 100))\n",
    "    \n",
    "    pl.xlabel(\"Componenti principali\")\n",
    "    pl.ylabel(\"Rapporto varianza spiegata\")\n",
    "    pl.legend(loc=\"center right\")\n",
    "    if despine:\n",
    "        sns.despine()\n",
    "        \n",
    "    pl.show()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varianza spiegata per dataset standardizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('poster')\n",
    "        \n",
    "    fig, ax = pl.subplots(ncols=1, figsize=(12, 10))\n",
    "    plot_explained_variance(pca, threshold=0.95, despine=True)\n",
    "    \n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'varianza-spiegata-std.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varianza spiegata per dataset non standardizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(style):\n",
    "    if export_images:\n",
    "        sns.set_context('poster')\n",
    "        \n",
    "    fig, ax = pl.subplots(ncols=1, figsize=(12, 10))\n",
    "    plot_explained_variance(pca_ns, threshold=0.95, despine=True)\n",
    "    \n",
    "    if export_images:\n",
    "        fig.savefig(img_folder + 'varianza-spiegata-non-std.png', bbox_inches='tight')\n",
    "        sns.set_context('notebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
